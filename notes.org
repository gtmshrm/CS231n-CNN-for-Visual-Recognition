[[http://cs231n.stanford.edu/syllabus.html][CS231n - CNNs for Visual Recognition]]

* [[http://cs231n.stanford.edu/syllabus.html][Notes to read after each lecture]]  

* Lec 1: Intro

* Lec 2: Image classification and the data-driven approach, k-nearest neighbor, Linear classification I

** kNN
*** Nearest Neighbour Classifier
**** Algo
***** Train -> Remember all training images and their labels
***** Predict -> Predict the label of most similar training image
**** How to compare? What is the distance metric?
     + There are many ways to compare. L1 distance (manhattan distance), L2 distance (Euclidean distance), etc. We'll use L1 distance
       L1 dist = sum(abs(image_from_training_examples-predication_image))
     + The choice of distance metric is a hyperparameter 
**** Scalability
     + How does the time required for classification increase with problem size?
       -> Linearly :(
          (CNN is completely opposite. Training is expensive but at test time, the computation happen in constant time)

*** k-Nearest Neighbour Classifier
    find the k nearest images, have them vote on the label (choice of k is a hyperparameter) 

*** Cross Validation (We would use k-fold CV)
   + The train-validation split which gives highest accuracy on validation set is considered
   + Used to get good performance and also selecting hyperparameters such as 
     choice of distance and value of k (of kNN Classifier)

**

** Summary
   + Image Classification: We are given a Training set of labeled images,
     asked to predict labels on test set. Common to report the Accuary
     of predictions (fraction of correctly predicted images)
     
   + We introduced the k-Nearest Neighbour Classifier, which predicts
     based on nearest images in the training set

   + We saw that the choice of distance and the value of k are hyperparameters
     that are tuned using a validation set or through cross-validation if the 
     size of the data is small

   + Once the best set of hyperparameters is chosen, the classifier is evaluated
     once on the test set, and reported as the performance of kNN on that data

** Linear Classification
   Wx + b
   consider m -> no. of classes and n -> length of feature vector
   W - mXn matrix (here, ith row is an independent classifier for ith class)
   x - nX1 vector
   b - mX1 vector
   
*** Interpreting a Linear Classifier (what does linear classifier do, in english)
    My answer:
    -> In a linear classifier, each row of weights tries to learn some linear pattern (information) 
       in the data of corresponding class. But data such as images are complex 
       and it is difficult for the classifier to extract linear information,
       making it less effective when used with such type of data. 
       For eg (shown in lecture): consider cifar10 in which each image is of 32X32X3.
       if the dataset has more red cars, then if we reshape (to 32X32X3) and visualize row 
       vector of weights (corresponding to that class - cars), then we see somthing red
       that almost resembles a car. This is beacuse the classifier thinks linearly
       and concludes that cars are mostly red. 
       Another eg shown in class is of a horse. if we visualize weight vector (row)
       corresponding to horse, we see something like 2 faced horse. This is because
       horse faces to both left and right in different images in cifar10
       This is where non-linear classifiers such as NN shines
       cuz intuitively, there might be different weights each independently looking out
       for left facing horse and right facing horse. Similarly, for the car example,
       there might be different weights where each independently looks out for different colored cars
   

* Lec 3: Linear Classification 2, Optimization

** Regularization
   There is a bug in loss function (Multi SVM Loss). Suppose that we found a W
   such that L = 0. Will W be unique? -> No
   W can be scaled which won't affect the loss. But that is not what we want
   We want the weights W to be nice and want it to be unique.
   "Regularization function measures niceness of W"
   Adding regularization tries to preserve uniqueness of W making it somewhat bounded
   so that W won't become arbitarily big or small.
   Regularization also prevents overfitting.
   
   -------------------------------------------
   final loss = original loss + regularization
   -------------------------------------------

   original loss : fights (or contributes) to make W fit the data. 
   regularization: fights (or contributes) to preserve niceness of W
   we need both of these properties. both fight with each other to
   give a reasonable W which may not fit training data that well but 
   will give a good performance on test set.
   in short, regularization term penalizes W to make it small
   and original loss term doesn't want W near 0 cuz then it 
   won't be able to classify well. at the end, we get a nice blend of both.

   L2 reg is also know as weight decay
   
** Softmax Classifer (Multinomial Logistic Regression)
   scores = unnormalized log probabilities of the classes

   so, to normalize these we use softmax since exp(log(x)) = x
   i.e exp(log(x))/sum(log(each of x's)) -> x/sum(all x's)

   -----------------------------------------------------------------
   How softmax works?

    unnormalized log probabilites 
            |||
             | exp 
            |||
     unnormalized probabilities
            |||
             | normalize
            |||
       probabilities
   -----------------------------------------------------------------
   
   loss = -log(softmax(correct class score))
   for probability near to 1, loss will be near to zero and 
   probability near to 0, loss will be near to infinity
   
   + (As a sanity check) while implementing, your first loss should be near or 
   rougly in the order of -log(1/n_classes) otherwise something is wrong
	
   
